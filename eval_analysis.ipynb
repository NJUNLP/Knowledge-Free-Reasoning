{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_data import load_llama_factory_data\n",
    "from typing import Optional\n",
    "import json\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from utils.model_registry import MODELS\n",
    "\n",
    "load_llama_factory_data = cache(load_llama_factory_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_language = ['en', 'de', 'fr', 'it', 'ar', 'he', 'ru', 'pl', 'ja', 'zh']\n",
    "\n",
    "\n",
    "def get_file_path(\n",
    "    test_data_name,\n",
    "    model_name,\n",
    "    train_data_name,\n",
    "    train_epoch,\n",
    "    train_lr='2e-4',\n",
    "    train_dropout='0',\n",
    "    train_type='lora-all',\n",
    "    template=None,\n",
    "    is_base_model=False,\n",
    ") -> str:\n",
    "    if template is None:\n",
    "        template = MODELS[model_name].default_template.replace('_', '-')\n",
    "    if is_base_model:\n",
    "        file_path = f'eval/outputs/{test_data_name}_{model_name}_{template}.json'\n",
    "    else:\n",
    "        file_path = f'eval/outputs/{test_data_name}_{model_name}-{train_data_name}{train_lr}-{train_dropout}-{train_epoch}-{train_type}_{template}.json'\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    task = 'arithmetic',\n",
    "    lang = 'EN',\n",
    "    model_name = 'llama-2-7b-chat',\n",
    "    train_lang = 'EN',\n",
    "    train_epoch = '1',\n",
    "    is_base_model = False,\n",
    "):\n",
    "    test_data_name = f'kfrd_{task}_{lang}_test'\n",
    "    train_data_name = f'kfrd-{task}-{train_lang}-train'\n",
    "    \n",
    "    train_lr = '2e-4'\n",
    "    train_dropout = '0'\n",
    "    train_type = 'lora-all'\n",
    "    template = MODELS[model_name].default_template.replace('_', '-')\n",
    "\n",
    "    file_path = get_file_path(\n",
    "        test_data_name=test_data_name,\n",
    "        model_name=model_name,\n",
    "        train_data_name=train_data_name,\n",
    "        train_lr=train_lr,\n",
    "        train_dropout=train_dropout,\n",
    "        train_epoch=train_epoch,\n",
    "        train_type=train_type,\n",
    "        template=template,\n",
    "        is_base_model=is_base_model,\n",
    "    )\n",
    "    \n",
    "    v = [t[1] for t in json.load(open(file_path))]\n",
    "    assert all(y in 'ABCD' for y in v)\n",
    "    return v\n",
    "\n",
    "\n",
    "def load_test_data(\n",
    "    task = 'arithmetic',\n",
    "    lang = 'EN',\n",
    "):\n",
    "    test_data_name = f'kfrd_{task}_{lang}_test'\n",
    "    testset = load_llama_factory_data(test_data_name)\n",
    "    return testset\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    task = 'arithmetic',\n",
    "    lang = 'EN',\n",
    "    model_name = 'llama-2-7b-chat',\n",
    "    train_lang = 'EN',\n",
    "    train_epoch = '1',\n",
    "    return_corrects=False,\n",
    "    is_base_model=False,\n",
    "):\n",
    "    testset = load_test_data(\n",
    "        task = task,\n",
    "        lang = lang,\n",
    "    )\n",
    "    hypset = load_data(\n",
    "        task = task,\n",
    "        lang = lang,\n",
    "        model_name = model_name,\n",
    "        train_lang = train_lang,\n",
    "        train_epoch = train_epoch,\n",
    "        is_base_model = is_base_model,\n",
    "    )\n",
    "    \n",
    "    corrects = [int(hyp == ref['response']) for hyp, ref in zip(hypset, testset)]\n",
    "    accuracy = sum(corrects) / len(testset)\n",
    "    if return_corrects:\n",
    "        return accuracy, corrects\n",
    "    else:\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "def evaluate_sqa_accuracy(\n",
    "    model_name: str, lang: str,\n",
    "    facts=False, return_corrects=False, is_base_model=False,\n",
    ") -> Optional[float]:\n",
    "    lang = lang.upper()\n",
    "    \n",
    "    if facts == 'one':\n",
    "        test_data_name = f'sqa_one_fact_dev_{lang}'\n",
    "        train_data_name = 'sqa-one-fact-train'\n",
    "    elif facts == 'two':\n",
    "        test_data_name = f'sqa_two_fact_dev_{lang}'\n",
    "        train_data_name = 'sqa-two-fact-train'\n",
    "    elif facts:\n",
    "        test_data_name = f'sqa_facts_dev_{lang}'\n",
    "        train_data_name = 'sqa-facts-train'\n",
    "    else:\n",
    "        test_data_name = f'sqa_dev_{lang}'\n",
    "        train_data_name = 'sqa-train'\n",
    "    \n",
    "    path = get_file_path(\n",
    "        test_data_name=test_data_name,\n",
    "        model_name=model_name,\n",
    "        train_data_name=train_data_name,\n",
    "        train_epoch='4',\n",
    "        is_base_model=is_base_model,\n",
    "    )\n",
    "    testset = load_llama_factory_data(\n",
    "        test_data_name,\n",
    "    )\n",
    "    hypset = json.load(open(path))\n",
    "    corrects = [int(hyp[1] == ref['output']) for hyp, ref in zip(hypset, testset)]\n",
    "    accuracy = sum(corrects) / len(testset)\n",
    "    if return_corrects:\n",
    "        return accuracy, corrects\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Before Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "facts_map = {\n",
    "    False: 'NF',\n",
    "    'one': 'WF-1',\n",
    "    'two': 'WF-2',\n",
    "    True: 'WF-all',\n",
    "}\n",
    "\n",
    "for facts in [False, 'one', 'two', True]:\n",
    "    d[facts_map[facts]] = {\n",
    "        lang: evaluate_sqa_accuracy('llama-2-7b-chat', lang, facts=facts, is_base_model=True)\n",
    "        for lang in all_language\n",
    "    }\n",
    "    \n",
    "base_accuracy_sqa_llama2_df = pd.DataFrame(d)\n",
    "# display(base_accuracy_sqa_llama2_df.T.mul(100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "display_name = {\n",
    "    'llama-2-7b-chat': 'LLaMA 2',\n",
    "    'qwen1.5-7b-chat': 'Qwen 1.5',\n",
    "    'bloomz-7b1-mt': 'BLOOMZ',\n",
    "    'mistral-7b-instruct-v0.1': 'Mistral',\n",
    "}\n",
    "\n",
    "for model_name in display_name:\n",
    "    d['StrategyQA NF', display_name[model_name]] = {\n",
    "        lang: evaluate_sqa_accuracy(model_name, lang, facts=False, is_base_model=True)\n",
    "        for lang in all_language\n",
    "    }\n",
    "\n",
    "for model_name in display_name:\n",
    "    d['StrategyQA WF', display_name[model_name]] = {\n",
    "        lang: evaluate_sqa_accuracy(model_name, lang, facts=True, is_base_model=True)\n",
    "        for lang in all_language\n",
    "    }\n",
    "    \n",
    "for model_name in display_name:\n",
    "    d['Arithmetic', display_name[model_name]] = {\n",
    "        lang: compute_accuracy(\n",
    "            'arithmetic', lang.upper(),\n",
    "            model_name=model_name, is_base_model=True,\n",
    "        )\n",
    "        for lang in all_language\n",
    "    }\n",
    "    \n",
    "for model_name in display_name:\n",
    "    d['Symbolic', display_name[model_name]] = {\n",
    "        lang: compute_accuracy(\n",
    "            'symbolic', lang.upper(),\n",
    "            model_name=model_name, is_base_model=True,\n",
    "        )\n",
    "        for lang in all_language\n",
    "    }\n",
    "    \n",
    "for model_name in display_name:\n",
    "    d['Logical', display_name[model_name]] = {\n",
    "        lang: compute_accuracy(\n",
    "            'logical', lang.upper(),\n",
    "            model_name=model_name, is_base_model=True,\n",
    "        )\n",
    "        for lang in all_language\n",
    "    }\n",
    "    \n",
    "base_accuracy_df = pd.DataFrame(d)\n",
    "# display(base_accuracy_df.T.mul(100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(ax: plt.Axes, df: pd.DataFrame, linestyle: str, alpha: float = 1.0):\n",
    "    df = df * 100\n",
    "    labels = list(df.index)\n",
    "    labels[0] = labels[0].upper()\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    delta = np.pi / 2\n",
    "    angles = np.linspace(0 + delta, 2 * np.pi + delta, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    angles += angles[:1]\n",
    "    angles = np.array(angles)\n",
    "    angles[angles >= 2 * np.pi] -= 2 * np.pi\n",
    "    \n",
    "    for model_name, color in zip(df.columns, colors):\n",
    "        values = df[model_name].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(\n",
    "            angles, values, label=model_name,\n",
    "            c=to_rgba(color, alpha=alpha), linestyle=linestyle,\n",
    "            linewidth=2, marker='.',\n",
    "        )\n",
    "    \n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    \n",
    "\n",
    "language = all_language\n",
    "display_name = {\n",
    "    'llama-2-7b-chat': 'LLaMA 2',\n",
    "    'qwen1.5-7b-chat': 'Qwen 1.5',\n",
    "    'bloomz-7b1-mt': 'BLOOMZ',\n",
    "    'mistral-7b-instruct-v0.1': 'Mistral',\n",
    "}\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), subplot_kw=dict(polar=True))\n",
    "ax: plt.Axes\n",
    "plot_radar(ax, base_accuracy_df['StrategyQA WF'], '-' , alpha=1.0)\n",
    "plot_radar(ax, base_accuracy_df['StrategyQA NF'], '--', alpha=1.0)\n",
    "fig.legend(list(display_name.values()), ncol=2, loc='upper center', bbox_to_anchor=(0.5, 0.0))\n",
    "# fig.tight_layout()\n",
    "fig.show()\n",
    "# fig.savefig('pic/sqa-acc-init.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StrategyQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(\n",
    "    ax: plt.Axes, df: pd.DataFrame,\n",
    "    alpha: float, colors: list[str],\n",
    "    linestyle: str = '-',\n",
    "):\n",
    "    df = df * 100\n",
    "    \n",
    "    labels = list(df.index)\n",
    "    labels[0] = labels[0].upper()\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    delta = np.pi / 2\n",
    "    angles = np.linspace(0 + delta, 2 * np.pi + delta, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    angles += angles[:1]\n",
    "    angles = np.array(angles)\n",
    "    angles[angles >= 2 * np.pi] -= 2 * np.pi\n",
    "    \n",
    "    assert len(colors) >= len(df.columns)\n",
    "    for model_name, color in zip(df.columns, colors):\n",
    "        values = df[model_name].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(\n",
    "            angles, values, label=model_name,\n",
    "            c=to_rgba(color, alpha=alpha), linestyle=linestyle,\n",
    "            linewidth=2, marker='.',\n",
    "        )\n",
    "    \n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sqa(facts=False):\n",
    "    data = {\n",
    "        display_name[model_name]: [\n",
    "            evaluate_sqa_accuracy(\n",
    "                model_name=model_name,\n",
    "                lang=lang,\n",
    "                facts=facts,\n",
    "                return_corrects=True\n",
    "            )[int(print_xltr)]\n",
    "            for lang in language\n",
    "        ]\n",
    "        for model_name in display_name\n",
    "    }\n",
    "    data['Language'] = language\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"Language\")\n",
    "    if print_xltr:\n",
    "        df = df.apply(\n",
    "            lambda series: pd.Series(\n",
    "                [\n",
    "                    sum(x & y for x, y in zip(series['en'], series[lang])) / sum(series['en'])\n",
    "                    for lang in series.index\n",
    "                ],\n",
    "                index=series.index,\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        df = df - 0.5\n",
    "        df = df.clip(lower=0.0) * 2\n",
    "    return df\n",
    "    \n",
    "    \n",
    "language = all_language\n",
    "display_name = {\n",
    "    'llama-2-7b-chat': 'LLaMA 2',\n",
    "    'qwen1.5-7b-chat': 'Qwen 1.5',\n",
    "    'bloomz-7b1-mt': 'BLOOMZ',\n",
    "    'mistral-7b-instruct-v0.1': 'Mistral',\n",
    "}\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "def draw():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4), subplot_kw=dict(polar=True))\n",
    "    ax: plt.Axes\n",
    "    plot_radar(ax, create_data_sqa(facts=True) , 1.0, colors, '-')\n",
    "    plot_radar(ax, create_data_sqa(facts=False), 1.0, colors, '--')\n",
    "    fig.legend(list(display_name.values()), loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0))\n",
    "    # fig.tight_layout()\n",
    "    fig.show()\n",
    "    # output_name = 'sqa-xltr.pdf' if print_xltr else 'sqa-acc.pdf'\n",
    "    # fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')\n",
    "    \n",
    "\n",
    "print_xltr = True ; draw() # XLTR\n",
    "print_xltr = False; draw() # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = {\n",
    "    'llama-2-7b-chat': 'LLaMA 2',\n",
    "}\n",
    "\n",
    "def draw():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4), subplot_kw=dict(polar=True))\n",
    "    ax: plt.Axes\n",
    "    plot_radar(ax, create_data_sqa(facts=False), 1.0, [colors[0]], '-')\n",
    "    plot_radar(ax, create_data_sqa(facts='one'), 1.0, [colors[1]], '-')\n",
    "    plot_radar(ax, create_data_sqa(facts='two'), 1.0, [colors[2]], '-')\n",
    "    plot_radar(ax, create_data_sqa(facts=True) , 1.0, [colors[3]], '-')\n",
    "    if not print_xltr:\n",
    "        plot_radar(ax, base_accuracy_sqa_llama2_df , 0.3, colors, '-')\n",
    "    fig.legend(['NF', 'WF-1', 'WF-2', 'WF-all'], loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0))\n",
    "    # fig.tight_layout()\n",
    "    fig.show()\n",
    "    # output_name = 'sqa-llama2-xltr.pdf' if print_xltr else 'sqa-llama2-acc.pdf'\n",
    "    # fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')\n",
    "    \n",
    "print_xltr = True ; draw() # XLTR\n",
    "print_xltr = False; draw() # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFRD Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(\n",
    "    task = 'arithmetic',\n",
    "    train_epoch = '1',\n",
    "):\n",
    "    def get_accuracy(model_name):\n",
    "        return [\n",
    "            compute_accuracy(\n",
    "                lang=lang.upper(),\n",
    "                model_name=model_name,\n",
    "                task=task,\n",
    "                train_epoch=train_epoch,\n",
    "                return_corrects=True,\n",
    "            )[int(print_xltr)]\n",
    "            for lang in language\n",
    "        ]\n",
    "\n",
    "    data = {\n",
    "        display_name[model_name]: get_accuracy(model_name)\n",
    "        for model_name in display_name\n",
    "    }\n",
    "    data['Language'] = language\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = df.set_index(\"Language\")\n",
    "    if print_xltr:\n",
    "        df = df.apply(\n",
    "            lambda series: pd.Series(\n",
    "                [\n",
    "                    sum(x & y for x, y in zip(series['en'], series[lang])) / sum(series['en'])\n",
    "                    for lang in series.index\n",
    "                ],\n",
    "                index=series.index,\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        df = df - 0.25\n",
    "        df = df.clip(lower=0.0) / (1 - 0.25)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def plot_radar(ax, df, title):\n",
    "    df *= 100\n",
    "    \n",
    "    labels = list(df.index)\n",
    "    labels[0] = labels[0].upper()\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    delta = np.pi / 2\n",
    "    angles = np.linspace(0 + delta, 2 * np.pi + delta, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    angles += angles[:1]\n",
    "    angles = np.array(angles)\n",
    "    angles[angles >= 2 * np.pi] -= 2 * np.pi\n",
    "    \n",
    "    for model, c in zip(df.columns, colors):\n",
    "        values = df[model].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(\n",
    "            angles, values, label=model,\n",
    "            c=c, linewidth=2, marker='.',\n",
    "        )\n",
    "        \n",
    "    if not print_xltr:\n",
    "        for model, c in zip(base_accuracy_df[title.split()[0]].columns, colors):\n",
    "            values = (base_accuracy_df[title.split()[0]][model] * 100).tolist()\n",
    "            values += values[:1]\n",
    "            ax.plot(\n",
    "                angles, values, label=model,\n",
    "                c=to_rgba(c, alpha=0.4), linewidth=2, marker='.',\n",
    "            )\n",
    "    \n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "language = all_language\n",
    "display_name = {\n",
    "    'llama-2-7b-chat': 'LLaMA 2',\n",
    "    'qwen1.5-7b-chat': 'Qwen 1.5',\n",
    "    'bloomz-7b1-mt': 'BLOOMZ',\n",
    "    'mistral-7b-instruct-v0.1': 'Mistral',\n",
    "}\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "\n",
    "def draw():\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4), squeeze=False, subplot_kw=dict(polar=True))\n",
    "    plot_radar(axs[0][0], create_data('arithmetic', train_epoch='4'), \"Arithmetic Reasoning\")\n",
    "    plot_radar(axs[0][1], create_data('symbolic'), \"Symbolic Reasoning\"  )\n",
    "    plot_radar(axs[0][2], create_data('logical'), \"Logical Reasoning\"   )\n",
    "    # fig.tight_layout()\n",
    "    fig.legend(list(display_name.values()), ncol=4, loc='lower center', bbox_to_anchor=(0.5, 0.0))\n",
    "    fig.show()\n",
    "    # output_name = 'main-xltr.pdf' if print_xltr else 'main-acc.pdf'\n",
    "    # fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')\n",
    "\n",
    "print_xltr = True ; draw() # XLTR\n",
    "print_xltr = False; draw() # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFRD Different Train Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(task : str) -> pd.DataFrame:\n",
    "    language = all_language\n",
    "    \n",
    "    def get_accuracy(train_lang):\n",
    "        return [\n",
    "            compute_accuracy(\n",
    "                lang=lang.upper(),\n",
    "                model_name='llama-2-7b-chat',\n",
    "                train_lang=train_lang,\n",
    "                task=task, \n",
    "                train_epoch='4' if task == 'arithmetic' else '1',\n",
    "                return_corrects=True,\n",
    "            )[int(print_xltr)]\n",
    "            for lang in language\n",
    "        ]\n",
    "\n",
    "    data = {\n",
    "        train_lang: get_accuracy(train_lang)\n",
    "        for train_lang in train_langs\n",
    "    }\n",
    "    data['Language'] = language\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index(\"Language\", inplace=True)\n",
    "    if print_xltr:\n",
    "        df = df.apply(\n",
    "            lambda series: pd.Series(\n",
    "                [\n",
    "                    sum(x & y for x, y in zip(series['en'], series[lang])) / sum(series['en'])\n",
    "                    for lang in series.index\n",
    "                ],\n",
    "                index=series.index,\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_radar(ax: plt.Axes, df: pd.DataFrame, title):\n",
    "    df = df * 100\n",
    "    labels = list(df.index)\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    delta = np.pi / 2\n",
    "    angles = np.linspace(0 + delta, 2 * np.pi + delta, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    angles += angles[:1]\n",
    "    angles = np.array(angles)\n",
    "    angles[angles >= 2 * np.pi] -= 2 * np.pi\n",
    "    for model in df.columns:\n",
    "        values = df[model].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(\n",
    "            angles, values, label=model,\n",
    "            linewidth=2, marker='.',\n",
    "        )\n",
    "    \n",
    "    ax.set_yticks([0, 25, 50, 75, 100])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)    \n",
    "    ax.set_title(title)\n",
    "\n",
    "def draw():\n",
    "    sns.set_palette('tab10')\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4), subplot_kw=dict(polar=True))\n",
    "    plot_radar(axs[0], create_data('arithmetic'), \"Arithmetic\")\n",
    "    plot_radar(axs[1], create_data('symbolic'  ), \"Symbolic\")\n",
    "    plot_radar(axs[2], create_data('logical'   ), \"Logical\")\n",
    "    # fig.tight_layout()\n",
    "    fig.legend(train_langs, ncol=5, loc='lower center', bbox_to_anchor=(0.5, 0))\n",
    "    fig.show()\n",
    "    # output_name = 'langs-xltr.pdf' if print_xltr else 'langs-acc.pdf'\n",
    "    # fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')\n",
    "\n",
    "train_langs = ['EN', 'DE', 'ZH', 'AR', 'HE']\n",
    "print_xltr = True ; draw() # XLTR\n",
    "print_xltr = False; draw() # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFRD SFT & CPT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_accuracy_for_model_and_lang_pair(\n",
    "    model_name: str,\n",
    "    train_lang: str,\n",
    "    lang: str,\n",
    "):\n",
    "    v = [\n",
    "        compute_accuracy(\n",
    "            task='arithmetic', train_epoch='4', lang=lang, train_lang=train_lang, model_name=model_name,\n",
    "        ),\n",
    "        compute_accuracy(\n",
    "            task='symbolic', lang=lang, train_lang=train_lang, model_name=model_name,\n",
    "        ),\n",
    "        compute_accuracy(\n",
    "            task='logical', lang=lang, train_lang=train_lang, model_name=model_name,\n",
    "        ),\n",
    "    ]\n",
    "    return sum(v) / 3\n",
    "    \n",
    "    \n",
    "def draw_pic3(ax: plt.Axes, model_names, alter_lang):\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.3\n",
    "    ax.bar(\n",
    "        x - 0.5 * width,\n",
    "        [\n",
    "            get_averaged_accuracy_for_model_and_lang_pair(model_name, 'EN', 'EN')\n",
    "            for model_name in model_names\n",
    "        ],\n",
    "        width, label='English',\n",
    "    )\n",
    "    ax.bar(\n",
    "        x + 0.5 * width,\n",
    "        [\n",
    "            get_averaged_accuracy_for_model_and_lang_pair(model_name, 'EN', alter_lang)\n",
    "            for model_name in model_names\n",
    "        ],\n",
    "        width, label='Hebrew',\n",
    "    )\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([labels[0]] + labels[2:])\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_title(alter_lang)\n",
    "\n",
    "\n",
    "def draw_pic4(ax: plt.Axes, model_names, alter_lang):\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.3\n",
    "    ax.bar(\n",
    "        x - 0.5 * width,\n",
    "        [\n",
    "            get_averaged_accuracy_for_model_and_lang_pair(model_name, 'EN', 'EN')\n",
    "            for model_name in model_names\n",
    "        ],\n",
    "        width, label='English',\n",
    "    )\n",
    "    ax.bar(\n",
    "        x + 0.5 * width,\n",
    "        [\n",
    "            get_averaged_accuracy_for_model_and_lang_pair(model_name, 'EN', alter_lang)\n",
    "            for model_name in model_names\n",
    "        ],\n",
    "        width, label='Arabic',\n",
    "    )\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_title(alter_lang)\n",
    "    \n",
    "    \n",
    "labels = [\n",
    "    'Vanilla',\n",
    "    'SFT',\n",
    "    'CPT',\n",
    "    'CPT+SFT',\n",
    "]\n",
    "    \n",
    "colors = ['#74a9cf', '#fc8d59']\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "    \n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4), sharey='all')\n",
    "draw_pic4(\n",
    "    axs[0],\n",
    "    [\n",
    "        'llama-2-7b-chat',\n",
    "        'llama-2-7b-chat-arabic-lora',\n",
    "        'sambalingo-arabic-base',\n",
    "        'sambalingo-arabic-chat',\n",
    "    ],\n",
    "    'AR',\n",
    ")\n",
    "draw_pic3(\n",
    "    axs[1],\n",
    "    ['mistral-7b-instruct-v0.1', 'dictalm-2', 'dictalm-2-instruct'], 'HE',\n",
    ")\n",
    "\n",
    "axs[1].legend(['English', 'Target Language'], loc='lower right', ncol=1)\n",
    "# fig.tight_layout()\n",
    "fig.show()\n",
    "# output_name = 'cptft-acc.pdf'\n",
    "# fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_ratio_for_model(\n",
    "    model_name: str,\n",
    "    alter_lang: str,\n",
    "):\n",
    "    v = [\n",
    "        [\n",
    "            compute_accuracy(\n",
    "                task='arithmetic', train_epoch='4',\n",
    "                lang=lang, model_name=model_name, return_corrects=True,\n",
    "            )[1],\n",
    "            compute_accuracy(\n",
    "                task='symbolic',\n",
    "                lang=lang, model_name=model_name, return_corrects=True,\n",
    "            )[1],\n",
    "            compute_accuracy(\n",
    "                task='logical',\n",
    "                lang=lang, model_name=model_name, return_corrects=True,\n",
    "            )[1],\n",
    "        ]\n",
    "        for lang in ('EN', alter_lang)\n",
    "    ]\n",
    "\n",
    "    \n",
    "    v = [\n",
    "        max(0, sum(x & y for x, y in zip(en_corrects, alter_corrects)) / sum(en_corrects) * 2 - 1)\n",
    "        for en_corrects, alter_corrects in zip(*v)\n",
    "    ]\n",
    "    return sum(v) / len(v)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 4), sharey='all')\n",
    "axs: list[plt.Axes]\n",
    "axs[0].bar(\n",
    "    list(range(4)),\n",
    "    [\n",
    "        get_transition_ratio_for_model(model_name, 'AR')\n",
    "        for model_name in [\n",
    "            'llama-2-7b-chat',\n",
    "            'llama-2-7b-chat-arabic-lora',\n",
    "            'sambalingo-arabic-base',\n",
    "            'sambalingo-arabic-chat',\n",
    "        ]\n",
    "    ],\n",
    "    width=0.5,\n",
    ")\n",
    "axs[0].set_xticks(list(range(4)))\n",
    "axs[0].set_xticklabels(labels)\n",
    "axs[0].set_title('AR')\n",
    "axs[0].grid(axis='y')\n",
    "\n",
    "axs[1].bar(\n",
    "    list(range(3)),\n",
    "    [\n",
    "        get_transition_ratio_for_model(model_name, 'HE')\n",
    "        for model_name in [\n",
    "            'mistral-7b-instruct-v0.1',\n",
    "            'dictalm-2', 'dictalm-2-instruct',\n",
    "        ]\n",
    "    ],\n",
    "    width=0.5,\n",
    ")\n",
    "axs[1].set_xticks(list(range(3)))\n",
    "axs[1].set_xticklabels(labels[0:1] + labels[2:])\n",
    "axs[1].set_title('HE')\n",
    "axs[1].grid(axis='y')\n",
    "\n",
    "# fig.tight_layout()\n",
    "fig.show()\n",
    "# output_name = 'cptft-xltr.pdf'\n",
    "# fig.savefig(os.path.join('pic', output_name), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
